import pytensor.tensor as pt
from pytensor.graph import node_rewriter
from pytensor.tensor import (
    broadcast_to,
    expand_dims,
    join,
    moveaxis,
    specify_shape,
    squeeze,
)
from pytensor.xtensor.basic import tensor_from_xtensor, xtensor_from_tensor
from pytensor.xtensor.rewriting.basic import register_lower_xtensor
from pytensor.xtensor.shape import (
    Concat,
    ExpandDims,
    Squeeze,
    Stack,
    Transpose,
    UnStack,
    XBroadcast,
)


@register_lower_xtensor
@node_rewriter(tracks=[Stack])
def lower_stack(fgraph, node):
    [x] = node.inputs
    batch_ndim = x.type.ndim - len(node.op.stacked_dims)
    stacked_axes = [
        i for i, dim in enumerate(x.type.dims) if dim in node.op.stacked_dims
    ]
    end = tuple(range(-len(stacked_axes), 0))

    x_tensor = tensor_from_xtensor(x)
    x_tensor_transposed = moveaxis(x_tensor, source=stacked_axes, destination=end)
    if batch_ndim == (x.type.ndim - 1):
        # This happens when we stack a "single" dimension, in this case all we need is the transpose
        # Note: If we have meaningful rewrites before lowering, consider canonicalizing this as a Transpose + Rename
        final_tensor = x_tensor_transposed
    else:
        final_shape = (*tuple(x_tensor_transposed.shape)[:batch_ndim], -1)
        final_tensor = x_tensor_transposed.reshape(final_shape)

    new_out = xtensor_from_tensor(final_tensor, dims=node.outputs[0].type.dims)
    return [new_out]


@register_lower_xtensor
@node_rewriter(tracks=[UnStack])
def lower_unstack(fgraph, node):
    x = node.inputs[0]
    unstacked_lengths = node.inputs[1:]
    axis_to_unstack = x.type.dims.index(node.op.old_dim_name)

    x_tensor = tensor_from_xtensor(x)
    x_tensor_transposed = moveaxis(x_tensor, source=[axis_to_unstack], destination=[-1])
    final_tensor = x_tensor_transposed.reshape(
        (*x_tensor_transposed.shape[:-1], *unstacked_lengths)
    )
    # Reintroduce any static shape information that was lost during the reshape
    final_tensor = specify_shape(final_tensor, node.outputs[0].type.shape)

    new_out = xtensor_from_tensor(final_tensor, dims=node.outputs[0].type.dims)
    return [new_out]


@register_lower_xtensor
@node_rewriter(tracks=[Concat])
def lower_concat(fgraph, node):
    out_dims = node.outputs[0].type.dims
    concat_dim = node.op.dim
    concat_axis = out_dims.index(concat_dim)

    # Convert input XTensors to Tensors and align batch dimensions
    tensor_inputs = []
    for inp in node.inputs:
        inp_dims = inp.type.dims
        order = [
            inp_dims.index(out_dim) if out_dim in inp_dims else "x"
            for out_dim in out_dims
        ]
        tensor_inp = tensor_from_xtensor(inp).dimshuffle(order)
        tensor_inputs.append(tensor_inp)

    # Broadcast non-concatenated dimensions of each input
    non_concat_shape = [None] * len(out_dims)
    for tensor_inp in tensor_inputs:
        # TODO: This is assuming the graph is correct and every non-concat dimension matches in shape at runtime
        # I'm running this as "shape_unsafe" to simplify the logic / returned graph
        for i, (bcast, sh) in enumerate(
            zip(tensor_inp.type.broadcastable, tensor_inp.shape)
        ):
            if bcast or i == concat_axis or non_concat_shape[i] is not None:
                continue
            non_concat_shape[i] = sh

    assert non_concat_shape.count(None) == 1

    bcast_tensor_inputs = []
    for tensor_inp in tensor_inputs:
        # We modify the concat_axis in place, as we don't need the list anywhere else
        non_concat_shape[concat_axis] = tensor_inp.shape[concat_axis]
        bcast_tensor_inputs.append(broadcast_to(tensor_inp, non_concat_shape))

    joined_tensor = join(concat_axis, *bcast_tensor_inputs)
    new_out = xtensor_from_tensor(joined_tensor, dims=out_dims)
    return [new_out]


@register_lower_xtensor
@node_rewriter(tracks=[Transpose])
def lower_transpose(fgraph, node):
    [x] = node.inputs
    # Use the final dimensions that were already computed in make_node
    out_dims = node.outputs[0].type.dims
    in_dims = x.type.dims

    # Compute the permutation based on the final dimensions
    perm = tuple(in_dims.index(d) for d in out_dims)
    x_tensor = tensor_from_xtensor(x)
    x_tensor_transposed = x_tensor.transpose(perm)
    new_out = xtensor_from_tensor(x_tensor_transposed, dims=out_dims)
    return [new_out]


@register_lower_xtensor
@node_rewriter([Squeeze])
def lower_squeeze(fgraph, node):
    """Rewrite Squeeze to tensor.squeeze."""
    [x] = node.inputs
    x_tensor = tensor_from_xtensor(x)
    x_dims = x.type.dims
    dims_to_remove = node.op.dims
    axes_to_squeeze = tuple(x_dims.index(d) for d in dims_to_remove)
    x_tensor_squeezed = squeeze(x_tensor, axis=axes_to_squeeze)

    new_out = xtensor_from_tensor(x_tensor_squeezed, dims=node.outputs[0].type.dims)
    return [new_out]


@register_lower_xtensor
@node_rewriter([ExpandDims])
def lower_expand_dims(fgraph, node):
    """Rewrite ExpandDims using tensor operations."""
    x, size = node.inputs
    out = node.outputs[0]

    # Convert inputs to tensors
    x_tensor = tensor_from_xtensor(x)
    size_tensor = tensor_from_xtensor(size)

    # Get the new dimension name and position
    new_axis = 0  # Always insert at front

    # Use tensor operations
    if out.type.shape[0] == 1:
        # Simple case: just expand with size 1
        result_tensor = expand_dims(x_tensor, new_axis)
    else:
        # Otherwise broadcast to the requested size
        result_tensor = broadcast_to(x_tensor, (size_tensor, *x_tensor.shape))

    # Preserve static shape information
    result_tensor = specify_shape(result_tensor, out.type.shape)

    # Convert result back to xtensor
    result = xtensor_from_tensor(result_tensor, dims=out.type.dims)
    return [result]


@register_lower_xtensor
@node_rewriter(tracks=[XBroadcast])
def lower_broadcast(fgraph, node):
    """Rewrite XBroadcast to tensor operations with symbolic shape support."""

    excluded_dims = node.op.exclude
    print(f"=== REWRITE DEBUG: excluded_dims = {excluded_dims} ===")

    if not excluded_dims:
        # simple case: broadcast all dimensions
        broadcast_dims = tuple(
            dim for dim in node.outputs[0].type.dims if dim not in excluded_dims
        )
        print(f"  No excluded dims - using broadcast_arrays")
        print(f"  broadcast_dims: {broadcast_dims}")

        inp_tensors = []
        for inp, out in zip(node.inputs, node.outputs, strict=True):
            inp_tensor = tensor_from_xtensor(inp)
            inp_dims = inp.type.dims
            order = tuple(
                inp_dims.index(dim) if dim in inp_dims else "x"
                for dim in broadcast_dims
            )
            print(f"    Input {inp.name}: dims={inp_dims}, order={order}")
            inp_tensor = inp_tensor.dimshuffle(order)
            print(f"    After dimshuffle: shape={inp_tensor.type.shape}")
            
            inp_tensors.append(inp_tensor)

        out_tensors = pt.broadcast_arrays(*inp_tensors)
        # print output shapes
        for out_tensor in out_tensors:
            print(f"    Output shape: {out_tensor.type.shape}")

    else:
        # complex case: broadcast only the dimensions that are not excluded
        print(f"  Has excluded dims - processing each tensor individually")
        print(f"  Excluded dims: {excluded_dims}")
        
        # Debug: show what make_node produced
        print(f"  Outputs from make_node:")
        for i, out in enumerate(node.outputs):
            print(f"    Output {i}: dims={out.type.dims}, shape={out.type.shape}")
        

        # Get the broadcast shape for non-excluded dimensions
        broadcast_shapes = []
        print(f"  Extracting non-excluded shapes from inputs:")
        
        # First, collect all non-excluded dimensions across all inputs
        all_non_excluded_dims = set()
        for x in node.inputs:
            non_excluded_dims = [d for d in x.type.dims if d not in excluded_dims]
            all_non_excluded_dims.update(non_excluded_dims)
        
        # Sort to ensure consistent ordering
        all_non_excluded_dims = sorted(all_non_excluded_dims)
        print(f"  All non-excluded dims: {all_non_excluded_dims}")
        
        for i, x in enumerate(node.inputs):
            # Get non-excluded dimensions and their shapes
            non_excluded_dims = [d for d in x.type.dims if d not in excluded_dims]
            non_excluded_shapes = [s for d, s in zip(x.type.dims, x.type.shape) if d not in excluded_dims]
            print(f"    Input {i}: dims={x.type.dims}, shape={x.type.shape}")
            print(f"    Non-excluded dims: {non_excluded_dims}")
            print(f"    Non-excluded shapes: {non_excluded_shapes}")
            
            # Create aligned shape with 1s for missing dimensions
            aligned_shape = []
            for dim in all_non_excluded_dims:
                if dim in non_excluded_dims:
                    idx = non_excluded_dims.index(dim)
                    aligned_shape.append(non_excluded_shapes[idx])
                else:
                    aligned_shape.append(1)  # Broadcastable dimension
            
            print(f"    Aligned shape: {aligned_shape}")
            broadcast_shapes.append(aligned_shape)
        
        print(f"  All aligned shapes: {broadcast_shapes}")
        
        # Compute the broadcast shape for non-excluded dims
        target_shape = pt.broadcast_shape(*broadcast_shapes, arrays_are_shapes=True)
        print(f"  Target shape for non-excluded dims: {target_shape}")
        
        out_tensors = []
        for inp, out in zip(node.inputs, node.outputs, strict=True):
            inp_tensor = tensor_from_xtensor(inp)
            inp_dims = inp.type.dims
            out_dims = out.type.dims
            print(f"    Input {inp.name}: dims={inp_dims}, shape={inp.type.shape}")
            print(f"    Output: dims={out_dims}, shape={out.type.shape}")

            # Align input tensor to output dims
            order = tuple(
                inp_dims.index(dim) if dim in inp_dims else "x" for dim in out_dims
            )
            print(f"    dimshuffle order: {order}")
            inp_tensor = inp_tensor.dimshuffle(order)
            print(f"    After dimshuffle: shape={inp_tensor.type.shape}")
            
            # Get excluded dims from this output
            excluded_dims_in_output = [d for d in out.type.dims if d in excluded_dims]
            excluded_shapes = [s for d, s in zip(out.type.dims, out.type.shape) if d in excluded_dims]
            print(f"    Excluded dims in output: {excluded_dims_in_output}")
            print(f"    Excluded shapes: {excluded_shapes}")
            
            # Combine target shape (for non-excluded) with excluded shapes
            final_shape = target_shape + tuple(excluded_shapes)
            print(f"    Final shape: {final_shape}")
            print(f"    Target shape: {target_shape}")
            print(f"    Excluded shapes: {excluded_shapes}")
            
            out_tensor = pt.broadcast_to(inp_tensor, final_shape)
            out_tensors.append(out_tensor)
            print(f"    After broadcast_to: shape={out_tensor.type.shape}")
            print(f"    ---")

    new_outs = [
        xtensor_from_tensor(out_tensor, dims=out.type.dims)
        for out_tensor, out in zip(out_tensors, node.outputs)
    ]
    return new_outs
