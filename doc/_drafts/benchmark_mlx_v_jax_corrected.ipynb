{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import pytensor\n",
        "import pytensor.tensor as pt\n",
        "from pytensor.compile.function import function\n",
        "from pytensor.compile.mode import Mode\n",
        "from pytensor.graph import RewriteDatabaseQuery\n",
        "from pytensor.link.jax import JAXLinker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure JAX to use float32 for consistency with MLX\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "# Set up PyTensor JAX mode\n",
        "jax_optimizer = RewriteDatabaseQuery(include=[\"jax\"], exclude=[])\n",
        "pytensor_jax_mode = \"JAX\"\n",
        "\n",
        "# Try to set up MLX mode\n",
        "try:\n",
        "    from pytensor.link.mlx import MLXLinker\n",
        "    import mlx.core as mx\n",
        "    mlx_optimizer = RewriteDatabaseQuery(include=[\"mlx\"], exclude=[])\n",
        "    pytensor_mlx_mode = \"MLX\"\n",
        "    MLX_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MLX_AVAILABLE = False\n",
        "\n",
        "def timer_jax(func, N=1000):\n",
        "    \"\"\"Time function execution with proper JAX synchronization, repeated N times\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        times = []\n",
        "        for _ in range(N):\n",
        "            start = time.perf_counter()\n",
        "            result = func(*args, **kwargs)\n",
        "            if hasattr(result, 'block_until_ready'):\n",
        "                result.block_until_ready()\n",
        "            elif isinstance(result, (list, tuple)):\n",
        "                for r in result:\n",
        "                    if hasattr(r, 'block_until_ready'):\n",
        "                        r.block_until_ready()\n",
        "            end = time.perf_counter()\n",
        "            times.append(end - start)\n",
        "        \n",
        "        mean_time = np.mean(times)\n",
        "        std_time = np.std(times)\n",
        "        return result, mean_time, std_time\n",
        "    return wrapper\n",
        "\n",
        "def timer_mlx(func, N=1000):\n",
        "    \"\"\"Time function execution with proper MLX synchronization, repeated N times\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        times = []\n",
        "        for _ in range(N):\n",
        "            start = time.perf_counter()\n",
        "            result = func(*args, **kwargs)\n",
        "            # For MLX, we need to use mx.eval() to force computation\n",
        "            if MLX_AVAILABLE:\n",
        "                if isinstance(result, (list, tuple)):\n",
        "                    mx.eval(*result)\n",
        "                else:\n",
        "                    mx.eval(result)\n",
        "            end = time.perf_counter()\n",
        "            times.append(end - start)\n",
        "        \n",
        "        mean_time = np.mean(times)\n",
        "        std_time = np.std(times)\n",
        "        return result, mean_time, std_time\n",
        "    return wrapper\n",
        "\n",
        "def run_benchmark(N=1000):\n",
        "    \"\"\"Run comprehensive benchmark comparing PyTensor JAX vs MLX backends\"\"\"\n",
        "    import pandas as pd\n",
        "    \n",
        "    sizes = [2, 4, 2000, 4000]\n",
        "    results = []\n",
        "    \n",
        "    print(f\"Running benchmarks with N={N} repetitions per test...\")\n",
        "    \n",
        "    for size in sizes:\n",
        "        print(f\"Testing {size}x{size} matrices...\")\n",
        "        \n",
        "        # Generate test matrices with fixed seed for reproducibility\n",
        "        np.random.seed(42)\n",
        "        A = np.random.randn(size, size).astype(np.float32)\n",
        "        B = np.random.randn(size, size).astype(np.float32)\n",
        "        C = np.random.randn(size, size).astype(np.float32)\n",
        "\n",
        "        pt_A = pt.matrix('A', dtype='float32')\n",
        "        pt_B = pt.matrix('B', dtype='float32')  \n",
        "        pt_C = pt.matrix('C', dtype='float32')\n",
        "        result = pt.dot(pt.dot(pt_A, pt_B), pt_C)\n",
        "\n",
        "\n",
        "        f_jax = function([pt_A, pt_B, pt_C], result, mode=pytensor_jax_mode, trust_input=True)\n",
        "        f_mlx = function([pt_A, pt_B, pt_C], result, mode=pytensor_mlx_mode, trust_input=True)\n",
        "        f_jax(A, B, C)\n",
        "        f_mlx(A, B, C)\n",
        "        \n",
        "        # === TEST 1: Matrix Multiplication Chain ===\n",
        "        # PyTensor + JAX backend\n",
        "        @timer_jax\n",
        "        def pytensor_jax_matmul():\n",
        "            return f_jax(A, B, C)\n",
        "        \n",
        "        # PyTensor + MLX backend\n",
        "        @timer_mlx\n",
        "        def pytensor_mlx_matmul():\n",
        "            if not MLX_AVAILABLE:\n",
        "                return None, float('inf'), 0\n",
        "            return f_mlx(A, B, C)\n",
        "        \n",
        "        # Run matrix multiplication test\n",
        "        _, jax_mean, jax_std = pytensor_jax_matmul()\n",
        "        try:\n",
        "            _, mlx_mean, mlx_std = pytensor_mlx_matmul()\n",
        "        except Exception as e:\n",
        "            print(f\"MLX matmul error: {e}\")\n",
        "            mlx_mean, mlx_std = float('inf'), 0\n",
        "        \n",
        "        # Calculate percentage improvement (positive = MLX is faster, negative = MLX is slower)\n",
        "        if mlx_mean != float('inf') and mlx_mean > 0:\n",
        "            speedup_percentage = ((jax_mean - mlx_mean) / jax_mean) * 100\n",
        "            speedup_str = f'{speedup_percentage:+.1f}%'\n",
        "        else:\n",
        "            speedup_str = 'N/A'\n",
        "        \n",
        "        results.append({\n",
        "            'Size': f'{size}x{size}',\n",
        "            'Operation': 'Matrix Chain (A @ B @ C)',\n",
        "            'PyTensor+JAX Mean (s)': f'{jax_mean:.6f}',\n",
        "            'PyTensor+JAX Std (s)': f'{jax_std:.6f}',\n",
        "            'PyTensor+MLX Mean (s)': f'{mlx_mean:.6f}' if mlx_mean != float('inf') else 'Error',\n",
        "            'PyTensor+MLX Std (s)': f'{mlx_std:.6f}' if mlx_mean != float('inf') else 'N/A',\n",
        "            'MLX Performance': speedup_str\n",
        "        })\n",
        "        \n",
        "        # === TEST 2: Element-wise Operations ===\n",
        "        # PyTensor + JAX\n",
        "        result = pt.sin(pt_A) + pt.cos(pt_B)\n",
        "        f_jax = function([pt_A, pt_B], result, mode=pytensor_jax_mode, trust_input=True)\n",
        "        f_mlx = function([pt_A, pt_B], result, mode=pytensor_mlx_mode, trust_input=True)\n",
        "        f_jax(A, B)\n",
        "        f_mlx(A, B)\n",
        "\n",
        "        @timer_jax\n",
        "        def pytensor_jax_elemwise():\n",
        "            return f_jax(A, B)\n",
        "        \n",
        "        # PyTensor + MLX\n",
        "        @timer_mlx\n",
        "        def pytensor_mlx_elemwise():\n",
        "            if not MLX_AVAILABLE:\n",
        "                return None, float('inf'), 0\n",
        "            return f_mlx(A, B)\n",
        "        \n",
        "        # Run element-wise test\n",
        "        _, jax_mean, jax_std = pytensor_jax_elemwise()\n",
        "        try:\n",
        "            _, mlx_mean, mlx_std = pytensor_mlx_elemwise()\n",
        "        except Exception as e:\n",
        "            print(f\"MLX elemwise error: {e}\")\n",
        "            mlx_mean, mlx_std = float('inf'), 0\n",
        "        \n",
        "        # Calculate percentage improvement\n",
        "        if mlx_mean != float('inf') and mlx_mean > 0:\n",
        "            speedup_percentage = ((jax_mean - mlx_mean) / jax_mean) * 100\n",
        "            speedup_str = f'{speedup_percentage:+.1f}%'\n",
        "        else:\n",
        "            speedup_str = 'N/A'\n",
        "        \n",
        "        results.append({\n",
        "            'Size': f'{size}x{size}',\n",
        "            'Operation': 'Element-wise (sin(A) + cos(B))',\n",
        "            'PyTensor+JAX Mean (s)': f'{jax_mean:.6f}',\n",
        "            'PyTensor+JAX Std (s)': f'{jax_std:.6f}',\n",
        "            'PyTensor+MLX Mean (s)': f'{mlx_mean:.6f}' if mlx_mean != float('inf') else 'Error',\n",
        "            'PyTensor+MLX Std (s)': f'{mlx_std:.6f}' if mlx_mean != float('inf') else 'N/A',\n",
        "            'MLX Performance': speedup_str\n",
        "        })\n",
        "        \n",
        "        # === TEST 3: Matrix Addition with Broadcasting ===\n",
        "        # PyTensor + JAX\n",
        "        result = pt_A + pt_B.T\n",
        "        f_jax = function([pt_A, pt_B], result, mode=pytensor_jax_mode, trust_input=True)\n",
        "        f_mlx = function([pt_A, pt_B], result, mode=pytensor_mlx_mode, trust_input=True)\n",
        "        f_jax(A, B)\n",
        "        f_mlx(A, B)\n",
        "        @timer_jax\n",
        "        def pytensor_jax_broadcast():\n",
        "            return f_jax(A, B)\n",
        "        \n",
        "        # PyTensor + MLX\n",
        "        @timer_mlx\n",
        "        def pytensor_mlx_broadcast():\n",
        "            if not MLX_AVAILABLE:\n",
        "                return None, float('inf'), 0\n",
        "            return f_mlx(A, B)\n",
        "        \n",
        "        # Run broadcasting test\n",
        "        _, jax_mean, jax_std = pytensor_jax_broadcast()\n",
        "        try:\n",
        "            _, mlx_mean, mlx_std = pytensor_mlx_broadcast()\n",
        "        except Exception as e:\n",
        "            print(f\"MLX broadcast error: {e}\")\n",
        "            mlx_mean, mlx_std = float('inf'), 0\n",
        "        \n",
        "        # Calculate percentage improvement\n",
        "        if mlx_mean != float('inf') and mlx_mean > 0:\n",
        "            speedup_percentage = ((jax_mean - mlx_mean) / jax_mean) * 100\n",
        "            speedup_str = f'{speedup_percentage:+.1f}%'\n",
        "        else:\n",
        "            speedup_str = 'N/A'\n",
        "        \n",
        "        results.append({\n",
        "            'Size': f'{size}x{size}',\n",
        "            'Operation': 'Broadcasting (A + B.T)',\n",
        "            'PyTensor+JAX Mean (s)': f'{jax_mean:.6f}',\n",
        "            'PyTensor+JAX Std (s)': f'{jax_std:.6f}',\n",
        "            'PyTensor+MLX Mean (s)': f'{mlx_mean:.6f}' if mlx_mean != float('inf') else 'Error',\n",
        "            'PyTensor+MLX Std (s)': f'{mlx_std:.6f}' if mlx_mean != float('inf') else 'N/A',\n",
        "            'MLX Performance': speedup_str\n",
        "        })\n",
        "    \n",
        "    # Create and display results table\n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n",
        "\n",
        "def main(N=1000):\n",
        "    \"\"\"Main benchmark execution\"\"\"\n",
        "    # Display system info\n",
        "    system_info = {\n",
        "        'JAX version': jax.__version__,\n",
        "        'PyTensor version': pytensor.__version__,\n",
        "        'MLX Available': 'Yes' if MLX_AVAILABLE else 'No',\n",
        "        'Platform': 'Apple Silicon' if MLX_AVAILABLE else 'Generic',\n",
        "        'Repetitions (N)': N\n",
        "    }\n",
        "    \n",
        "    if MLX_AVAILABLE:\n",
        "        system_info['MLX version'] = mx.__version__\n",
        "    \n",
        "    import pandas as pd\n",
        "    info_df = pd.DataFrame([system_info])\n",
        "    \n",
        "    # Then run benchmarks\n",
        "    results_df = run_benchmark(N=N)\n",
        "    \n",
        "    return info_df, results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running benchmarks with N=150 repetitions per test...\n",
            "Testing 2x2 matrices...\n",
            "Testing 4x4 matrices...\n",
            "Testing 2000x2000 matrices...\n",
            "Testing 4000x4000 matrices...\n"
          ]
        }
      ],
      "source": [
        "iteration=150\n",
        "_, results = main(N=iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Benchmark Results over 150 repetitions:\n",
            "     Size                      Operation PyTensor+JAX Mean (s) PyTensor+JAX Std (s) PyTensor+MLX Mean (s) PyTensor+MLX Std (s) MLX Performance\n",
            "      2x2       Matrix Chain (A @ B @ C)              0.000009             0.000002              0.000311             0.000266        -3277.7%\n",
            "      2x2 Element-wise (sin(A) + cos(B))              0.000008             0.000003              0.000233             0.000105        -2830.3%\n",
            "      2x2         Broadcasting (A + B.T)              0.000007             0.000003              0.000253             0.000151        -3429.1%\n",
            "      4x4       Matrix Chain (A @ B @ C)              0.000011             0.000008              0.000285             0.000111        -2537.7%\n",
            "      4x4 Element-wise (sin(A) + cos(B))              0.000007             0.000001              0.000235             0.000124        -3217.0%\n",
            "      4x4         Broadcasting (A + B.T)              0.000007             0.000002              0.000202             0.000077        -2755.8%\n",
            "2000x2000       Matrix Chain (A @ B @ C)              0.024714             0.000919              0.004166             0.003531          +83.1%\n",
            "2000x2000 Element-wise (sin(A) + cos(B))              0.009464             0.000417              0.000844             0.000284          +91.1%\n",
            "2000x2000         Broadcasting (A + B.T)              0.000690             0.000022              0.000821             0.000093          -19.0%\n",
            "4000x4000       Matrix Chain (A @ B @ C)              0.196587             0.008780              0.027411             0.001132          +86.1%\n",
            "4000x4000 Element-wise (sin(A) + cos(B))              0.037744             0.001247              0.003355             0.000467          +91.1%\n",
            "4000x4000         Broadcasting (A + B.T)              0.012233             0.000421              0.003323             0.000370          +72.8%\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nBenchmark Results over {iteration} repetitions:\")\n",
        "print(results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Additional timing analysis - separate compilation vs execution time\n",
        "# if MLX_AVAILABLE:\n",
        "#     print(\"\\n=== Detailed MLX Timing Analysis ===\")\n",
        "    \n",
        "#     # Test with medium-sized matrix\n",
        "#     np.random.seed(42)\n",
        "#     A = np.random.randn(512, 512).astype(np.float32)\n",
        "#     B = np.random.randn(512, 512).astype(np.float32)\n",
        "#     C = np.random.randn(512, 512).astype(np.float32)\n",
        "    \n",
        "#     # Create PyTensor function (compilation time)\n",
        "#     start = time.perf_counter()\n",
        "#     pt_A = pt.matrix('A', dtype='float32')\n",
        "#     pt_B = pt.matrix('B', dtype='float32')\n",
        "#     pt_C = pt.matrix('C', dtype='float32')\n",
        "#     result_expr = pt_A @ pt_B @ pt_C\n",
        "#     f_mlx = function([pt_A, pt_B, pt_C], result_expr, mode=pytensor_mlx_mode)\n",
        "#     compilation_time = time.perf_counter() - start\n",
        "    \n",
        "#     # First execution (may include additional compilation/optimization)\n",
        "#     start = time.perf_counter()\n",
        "#     result = f_mlx(A, B, C)\n",
        "#     mx.eval(result)  # Force evaluation\n",
        "#     first_exec_time = time.perf_counter() - start\n",
        "    \n",
        "#     # Subsequent executions (should be faster)\n",
        "#     exec_times = []\n",
        "#     for _ in range(1000):\n",
        "#         start = time.perf_counter()\n",
        "#         result = f_mlx(A, B, C)\n",
        "#         mx.eval(result)\n",
        "#         exec_times.append(time.perf_counter() - start)\n",
        "    \n",
        "#     avg_exec_time = np.mean(exec_times)\n",
        "#     std_exec_time = np.std(exec_times)\n",
        "    \n",
        "#     print(f\"Compilation time: {compilation_time:.4f}s\")\n",
        "#     print(f\"First execution: {first_exec_time:.4f}s\")\n",
        "#     print(f\"Average execution (5 runs): {avg_exec_time:.4f}s Â± {std_exec_time:.4f}s\")\n",
        "#     print(f\"Individual execution times: {[f'{t:.4f}' for t in exec_times]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlx_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
