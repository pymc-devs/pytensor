{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import pytensor\n",
        "import pytensor.tensor as pt\n",
        "from pytensor.compile.function import function\n",
        "from pytensor.compile.mode import Mode\n",
        "from pytensor.graph import RewriteDatabaseQuery\n",
        "from pytensor.link.jax import JAXLinker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure JAX to use float32 for consistency with MLX\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "# Set up PyTensor JAX mode\n",
        "jax_optimizer = RewriteDatabaseQuery(include=[\"jax\"], exclude=[])\n",
        "pytensor_jax_mode = Mode(linker=JAXLinker(), optimizer=jax_optimizer)\n",
        "\n",
        "# Try to set up MLX mode\n",
        "try:\n",
        "    from pytensor.link.mlx import MLXLinker\n",
        "    import mlx.core as mx\n",
        "    mlx_optimizer = RewriteDatabaseQuery(include=[\"mlx\"], exclude=[])\n",
        "    pytensor_mlx_mode = Mode(linker=MLXLinker(), optimizer=mlx_optimizer)\n",
        "    MLX_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MLX_AVAILABLE = False\n",
        "\n",
        "def timer_jax(func, N=1000):\n",
        "    \"\"\"Time function execution with proper JAX synchronization, repeated N times\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        times = []\n",
        "        for _ in range(N):\n",
        "            start = time.perf_counter()\n",
        "            result = func(*args, **kwargs)\n",
        "            if hasattr(result, 'block_until_ready'):\n",
        "                result.block_until_ready()\n",
        "            elif isinstance(result, (list, tuple)):\n",
        "                for r in result:\n",
        "                    if hasattr(r, 'block_until_ready'):\n",
        "                        r.block_until_ready()\n",
        "            end = time.perf_counter()\n",
        "            times.append(end - start)\n",
        "        \n",
        "        mean_time = np.mean(times)\n",
        "        std_time = np.std(times)\n",
        "        return result, mean_time, std_time\n",
        "    return wrapper\n",
        "\n",
        "def timer_mlx(func, N=1000):\n",
        "    \"\"\"Time function execution with proper MLX synchronization, repeated N times\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        times = []\n",
        "        for _ in range(N):\n",
        "            start = time.perf_counter()\n",
        "            result = func(*args, **kwargs)\n",
        "            # For MLX, we need to use mx.eval() to force computation\n",
        "            if MLX_AVAILABLE:\n",
        "                if isinstance(result, (list, tuple)):\n",
        "                    mx.eval(*result)\n",
        "                else:\n",
        "                    mx.eval(result)\n",
        "            end = time.perf_counter()\n",
        "            times.append(end - start)\n",
        "        \n",
        "        mean_time = np.mean(times)\n",
        "        std_time = np.std(times)\n",
        "        return result, mean_time, std_time\n",
        "    return wrapper\n",
        "\n",
        "def run_benchmark(N=1000):\n",
        "    \"\"\"Run comprehensive benchmark comparing PyTensor JAX vs MLX backends\"\"\"\n",
        "    import pandas as pd\n",
        "    \n",
        "    sizes = [128, 256, 512, 1024]\n",
        "    results = []\n",
        "    \n",
        "    print(f\"Running benchmarks with N={N} repetitions per test...\")\n",
        "    \n",
        "    for size in sizes:\n",
        "        print(f\"Testing {size}x{size} matrices...\")\n",
        "        \n",
        "        # Generate test matrices with fixed seed for reproducibility\n",
        "        np.random.seed(42)\n",
        "        A = np.random.randn(size, size).astype(np.float32)\n",
        "        B = np.random.randn(size, size).astype(np.float32)\n",
        "        C = np.random.randn(size, size).astype(np.float32)\n",
        "        \n",
        "        # === TEST 1: Matrix Multiplication Chain ===\n",
        "        # PyTensor + JAX backend\n",
        "        @timer_jax\n",
        "        def pytensor_jax_matmul():\n",
        "            pt_A = pt.matrix('A', dtype='float32')\n",
        "            pt_B = pt.matrix('B', dtype='float32')  \n",
        "            pt_C = pt.matrix('C', dtype='float32')\n",
        "            result = pt.dot(pt.dot(pt_A, pt_B), pt_C)\n",
        "            f = function([pt_A, pt_B, pt_C], result, mode=pytensor_jax_mode)\n",
        "            return f(A, B, C)\n",
        "        \n",
        "        # PyTensor + MLX backend\n",
        "        @timer_mlx\n",
        "        def pytensor_mlx_matmul():\n",
        "            if not MLX_AVAILABLE:\n",
        "                return None, float('inf'), 0\n",
        "            pt_A = pt.matrix('A', dtype='float32')\n",
        "            pt_B = pt.matrix('B', dtype='float32')\n",
        "            pt_C = pt.matrix('C', dtype='float32')\n",
        "            result = pt_A @ pt_B @ pt_C\n",
        "            f = function([pt_A, pt_B, pt_C], result, mode=pytensor_mlx_mode)\n",
        "            return f(A, B, C)\n",
        "        \n",
        "        # Run matrix multiplication test\n",
        "        _, jax_mean, jax_std = pytensor_jax_matmul()\n",
        "        try:\n",
        "            _, mlx_mean, mlx_std = pytensor_mlx_matmul()\n",
        "        except Exception as e:\n",
        "            print(f\"MLX matmul error: {e}\")\n",
        "            mlx_mean, mlx_std = float('inf'), 0\n",
        "        \n",
        "        results.append({\n",
        "            'Size': f'{size}x{size}',\n",
        "            'Operation': 'Matrix Chain (A @ B @ C)',\n",
        "            'PyTensor+JAX Mean (s)': f'{jax_mean:.6f}',\n",
        "            'PyTensor+JAX Std (s)': f'{jax_std:.6f}',\n",
        "            'PyTensor+MLX Mean (s)': f'{mlx_mean:.6f}' if mlx_mean != float('inf') else 'Error',\n",
        "            'PyTensor+MLX Std (s)': f'{mlx_std:.6f}' if mlx_mean != float('inf') else 'N/A',\n",
        "            'MLX Speedup': f'{jax_mean/mlx_mean:.2f}x' if mlx_mean != float('inf') and mlx_mean > 0 else 'N/A'\n",
        "        })\n",
        "        \n",
        "        # === TEST 2: Element-wise Operations ===\n",
        "        # PyTensor + JAX\n",
        "        @timer_jax\n",
        "        def pytensor_jax_elemwise():\n",
        "            pt_A = pt.matrix('A', dtype='float32')\n",
        "            pt_B = pt.matrix('B', dtype='float32')\n",
        "            result = pt.sin(pt_A) + pt.cos(pt_B)\n",
        "            f = function([pt_A, pt_B], result, mode=pytensor_jax_mode)\n",
        "            return f(A, B)\n",
        "        \n",
        "        # PyTensor + MLX\n",
        "        @timer_mlx\n",
        "        def pytensor_mlx_elemwise():\n",
        "            if not MLX_AVAILABLE:\n",
        "                return None, float('inf'), 0\n",
        "            pt_A = pt.matrix('A', dtype='float32')\n",
        "            pt_B = pt.matrix('B', dtype='float32')\n",
        "            result = pt.sin(pt_A) + pt.cos(pt_B)\n",
        "            f = function([pt_A, pt_B], result, mode=pytensor_mlx_mode)\n",
        "            return f(A, B)\n",
        "        \n",
        "        # Run element-wise test\n",
        "        _, jax_mean, jax_std = pytensor_jax_elemwise()\n",
        "        try:\n",
        "            _, mlx_mean, mlx_std = pytensor_mlx_elemwise()\n",
        "        except Exception as e:\n",
        "            print(f\"MLX elemwise error: {e}\")\n",
        "            mlx_mean, mlx_std = float('inf'), 0\n",
        "        \n",
        "        results.append({\n",
        "            'Size': f'{size}x{size}',\n",
        "            'Operation': 'Element-wise (sin(A) + cos(B))',\n",
        "            'PyTensor+JAX Mean (s)': f'{jax_mean:.6f}',\n",
        "            'PyTensor+JAX Std (s)': f'{jax_std:.6f}',\n",
        "            'PyTensor+MLX Mean (s)': f'{mlx_mean:.6f}' if mlx_mean != float('inf') else 'Error',\n",
        "            'PyTensor+MLX Std (s)': f'{mlx_std:.6f}' if mlx_mean != float('inf') else 'N/A',\n",
        "            'MLX Speedup': f'{jax_mean/mlx_mean:.2f}x' if mlx_mean != float('inf') and mlx_mean > 0 else 'N/A'\n",
        "        })\n",
        "        \n",
        "        # === TEST 3: Matrix Addition with Broadcasting ===\n",
        "        # PyTensor + JAX\n",
        "        @timer_jax\n",
        "        def pytensor_jax_broadcast():\n",
        "            pt_A = pt.matrix('A', dtype='float32')\n",
        "            pt_B = pt.matrix('B', dtype='float32')\n",
        "            result = pt_A + pt_B.T\n",
        "            f = function([pt_A, pt_B], result, mode=pytensor_jax_mode)\n",
        "            return f(A, B)\n",
        "        \n",
        "        # PyTensor + MLX\n",
        "        @timer_mlx\n",
        "        def pytensor_mlx_broadcast():\n",
        "            if not MLX_AVAILABLE:\n",
        "                return None, float('inf'), 0\n",
        "            pt_A = pt.matrix('A', dtype='float32')\n",
        "            pt_B = pt.matrix('B', dtype='float32')\n",
        "            result = pt_A + pt_B.T\n",
        "            f = function([pt_A, pt_B], result, mode=pytensor_mlx_mode)\n",
        "            return f(A, B)\n",
        "        \n",
        "        # Run broadcasting test\n",
        "        _, jax_mean, jax_std = pytensor_jax_broadcast()\n",
        "        try:\n",
        "            _, mlx_mean, mlx_std = pytensor_mlx_broadcast()\n",
        "        except Exception as e:\n",
        "            print(f\"MLX broadcast error: {e}\")\n",
        "            mlx_mean, mlx_std = float('inf'), 0\n",
        "        \n",
        "        results.append({\n",
        "            'Size': f'{size}x{size}',\n",
        "            'Operation': 'Broadcasting (A + B.T)',\n",
        "            'PyTensor+JAX Mean (s)': f'{jax_mean:.6f}',\n",
        "            'PyTensor+JAX Std (s)': f'{jax_std:.6f}',\n",
        "            'PyTensor+MLX Mean (s)': f'{mlx_mean:.6f}' if mlx_mean != float('inf') else 'Error',\n",
        "            'PyTensor+MLX Std (s)': f'{mlx_std:.6f}' if mlx_mean != float('inf') else 'N/A',\n",
        "            'MLX Speedup': f'{jax_mean/mlx_mean:.2f}x' if mlx_mean != float('inf') and mlx_mean > 0 else 'N/A'\n",
        "        })\n",
        "    \n",
        "    # Create and display results table\n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n",
        "\n",
        "def verify_computation_correctness():\n",
        "    \"\"\"Verify that JAX and MLX backends produce the same results\"\"\"\n",
        "    if not MLX_AVAILABLE:\n",
        "        print(\"MLX not available, skipping correctness check\")\n",
        "        return\n",
        "    \n",
        "    print(\"Verifying computational correctness...\")\n",
        "    \n",
        "    # Test with small matrices\n",
        "    np.random.seed(42)\n",
        "    A = np.random.randn(4, 4).astype(np.float32)\n",
        "    B = np.random.randn(4, 4).astype(np.float32)\n",
        "    C = np.random.randn(4, 4).astype(np.float32)\n",
        "    \n",
        "    # Test matrix multiplication\n",
        "    pt_A = pt.matrix('A', dtype='float32')\n",
        "    pt_B = pt.matrix('B', dtype='float32')\n",
        "    pt_C = pt.matrix('C', dtype='float32')\n",
        "    result_expr = pt_A @ pt_B @ pt_C\n",
        "    \n",
        "    f_jax = function([pt_A, pt_B, pt_C], result_expr, mode=pytensor_jax_mode)\n",
        "    f_mlx = function([pt_A, pt_B, pt_C], result_expr, mode=pytensor_mlx_mode)\n",
        "    \n",
        "    result_jax = f_jax(A, B, C)\n",
        "    result_mlx = f_mlx(A, B, C)\n",
        "    \n",
        "    # Force MLX evaluation\n",
        "    mx.eval(result_mlx)\n",
        "    \n",
        "    # Convert to numpy for comparison\n",
        "    if hasattr(result_jax, 'block_until_ready'):\n",
        "        result_jax.block_until_ready()\n",
        "    \n",
        "    diff = np.abs(np.array(result_jax) - np.array(result_mlx)).max()\n",
        "    print(f\"Max difference between JAX and MLX results: {diff:.2e}\")\n",
        "    \n",
        "    if diff < 1e-5:\n",
        "        print(\"✅ Results match within tolerance\")\n",
        "    else:\n",
        "        print(\"❌ Results differ significantly\")\n",
        "    \n",
        "    return diff\n",
        "\n",
        "def main(N=1000):\n",
        "    \"\"\"Main benchmark execution\"\"\"\n",
        "    # Display system info\n",
        "    system_info = {\n",
        "        'JAX version': jax.__version__,\n",
        "        'PyTensor version': pytensor.__version__,\n",
        "        'MLX Available': 'Yes' if MLX_AVAILABLE else 'No',\n",
        "        'Platform': 'Apple Silicon' if MLX_AVAILABLE else 'Generic',\n",
        "        'Repetitions (N)': N\n",
        "    }\n",
        "    \n",
        "    if MLX_AVAILABLE:\n",
        "        system_info['MLX version'] = mx.__version__\n",
        "    \n",
        "    import pandas as pd\n",
        "    info_df = pd.DataFrame([system_info])\n",
        "    \n",
        "    # First verify correctness\n",
        "    verify_computation_correctness()\n",
        "    \n",
        "    # Then run benchmarks\n",
        "    results_df = run_benchmark(N=N)\n",
        "    \n",
        "    return info_df, results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying computational correctness...\n",
            "Max difference between JAX and MLX results: 0.00e+00\n",
            "✅ Results match within tolerance\n",
            "Running benchmarks with N=20 repetitions per test...\n",
            "Testing 128x128 matrices...\n"
          ]
        }
      ],
      "source": [
        "iteration=20\n",
        "_, results = main(N=iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Benchmark Results over 1000 repetitions:\n",
            "     Size                      Operation PyTensor+JAX Mean (s) PyTensor+JAX Std (s) PyTensor+MLX Mean (s) PyTensor+MLX Std (s) MLX Speedup\n",
            "  128x128       Matrix Chain (A @ B @ C)              0.005700             0.002127              0.001215             0.000497       4.69x\n",
            "  128x128 Element-wise (sin(A) + cos(B))              0.008280             0.002158              0.000876             0.000451       9.45x\n",
            "  128x128         Broadcasting (A + B.T)              0.008083             0.002485              0.000861             0.000207       9.39x\n",
            "  256x256       Matrix Chain (A @ B @ C)              0.005705             0.002307              0.001085             0.000210       5.26x\n",
            "  256x256 Element-wise (sin(A) + cos(B))              0.009794             0.001994              0.000998             0.001895       9.82x\n",
            "  256x256         Broadcasting (A + B.T)              0.010467             0.002573              0.001056             0.000578       9.91x\n",
            "  512x512       Matrix Chain (A @ B @ C)              0.006898             0.002576              0.001300             0.000391       5.31x\n",
            "  512x512 Element-wise (sin(A) + cos(B))              0.010997             0.002435              0.000976             0.000584      11.27x\n",
            "  512x512         Broadcasting (A + B.T)              0.009730             0.002690              0.000968             0.000315      10.05x\n",
            "1024x1024       Matrix Chain (A @ B @ C)              0.010941             0.002035              0.001735             0.000302       6.31x\n",
            "1024x1024 Element-wise (sin(A) + cos(B))              0.013936             0.003774              0.001103             0.000253      12.64x\n",
            "1024x1024         Broadcasting (A + B.T)              0.011153             0.002297              0.001084             0.000242      10.29x\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nBenchmark Results over {iteration} repetitions:\")\n",
        "print(results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Detailed MLX Timing Analysis ===\n",
            "Compilation time: 0.0020s\n",
            "First execution: 0.0061s\n",
            "Average execution (5 runs): 0.0004s ± 0.0001s\n",
            "Individual execution times: ['0.0007', '0.0005', '0.0006', '0.0008', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0005', '0.0006', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0005', '0.0004', '0.0004', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0007', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0012', '0.0010', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0005', '0.0005', '0.0005', '0.0006', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0008', '0.0006', '0.0006', '0.0005', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0006', '0.0006', '0.0006', '0.0006', '0.0007', '0.0006', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0006', '0.0005', '0.0004', '0.0005', '0.0005', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0005', '0.0005', '0.0006', '0.0005', '0.0005', '0.0004', '0.0005', '0.0005', '0.0008', '0.0006', '0.0006', '0.0007', '0.0006', '0.0006', '0.0006', '0.0006', '0.0007', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0009', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0008', '0.0007', '0.0005', '0.0005', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0007', '0.0007', '0.0006', '0.0008', '0.0007', '0.0006', '0.0006', '0.0007', '0.0007', '0.0006', '0.0006', '0.0007', '0.0007', '0.0007', '0.0007', '0.0006', '0.0007', '0.0007', '0.0007', '0.0007', '0.0008', '0.0007', '0.0007', '0.0007', '0.0006', '0.0007', '0.0008', '0.0007', '0.0008', '0.0007', '0.0007', '0.0008', '0.0007', '0.0007', '0.0007', '0.0007', '0.0008', '0.0006', '0.0007', '0.0007', '0.0008', '0.0007', '0.0007', '0.0007', '0.0007', '0.0007', '0.0007', '0.0006', '0.0007', '0.0007', '0.0007', '0.0008', '0.0007', '0.0007', '0.0007', '0.0007', '0.0007', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0009', '0.0007', '0.0006', '0.0006', '0.0006', '0.0009', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0006', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0004', '0.0005', '0.0007', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0007', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0003', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0005', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0005', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0005', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0005', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0005', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003']\n"
          ]
        }
      ],
      "source": [
        "# Additional timing analysis - separate compilation vs execution time\n",
        "if MLX_AVAILABLE:\n",
        "    print(\"\\n=== Detailed MLX Timing Analysis ===\")\n",
        "    \n",
        "    # Test with medium-sized matrix\n",
        "    np.random.seed(42)\n",
        "    A = np.random.randn(512, 512).astype(np.float32)\n",
        "    B = np.random.randn(512, 512).astype(np.float32)\n",
        "    C = np.random.randn(512, 512).astype(np.float32)\n",
        "    \n",
        "    # Create PyTensor function (compilation time)\n",
        "    start = time.perf_counter()\n",
        "    pt_A = pt.matrix('A', dtype='float32')\n",
        "    pt_B = pt.matrix('B', dtype='float32')\n",
        "    pt_C = pt.matrix('C', dtype='float32')\n",
        "    result_expr = pt_A @ pt_B @ pt_C\n",
        "    f_mlx = function([pt_A, pt_B, pt_C], result_expr, mode=pytensor_mlx_mode)\n",
        "    compilation_time = time.perf_counter() - start\n",
        "    \n",
        "    # First execution (may include additional compilation/optimization)\n",
        "    start = time.perf_counter()\n",
        "    result = f_mlx(A, B, C)\n",
        "    mx.eval(result)  # Force evaluation\n",
        "    first_exec_time = time.perf_counter() - start\n",
        "    \n",
        "    # Subsequent executions (should be faster)\n",
        "    exec_times = []\n",
        "    for _ in range(1000):\n",
        "        start = time.perf_counter()\n",
        "        result = f_mlx(A, B, C)\n",
        "        mx.eval(result)\n",
        "        exec_times.append(time.perf_counter() - start)\n",
        "    \n",
        "    avg_exec_time = np.mean(exec_times)\n",
        "    std_exec_time = np.std(exec_times)\n",
        "    \n",
        "    print(f\"Compilation time: {compilation_time:.4f}s\")\n",
        "    print(f\"First execution: {first_exec_time:.4f}s\")\n",
        "    print(f\"Average execution (5 runs): {avg_exec_time:.4f}s ± {std_exec_time:.4f}s\")\n",
        "    print(f\"Individual execution times: {[f'{t:.4f}' for t in exec_times]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlx_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
