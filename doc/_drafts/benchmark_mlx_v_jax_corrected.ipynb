{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import pytensor\n",
        "import pytensor.tensor as pt\n",
        "from pytensor.compile.function import function\n",
        "from pytensor.compile.mode import Mode\n",
        "from pytensor.graph import RewriteDatabaseQuery\n",
        "from pytensor.link.jax import JAXLinker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure JAX to use float32 for consistency with MLX\n",
        "jax.config.update(\"jax_enable_x64\", False)\n",
        "\n",
        "# Set up PyTensor JAX mode\n",
        "jax_optimizer = RewriteDatabaseQuery(include=[\"jax\"], exclude=[])\n",
        "pytensor_jax_mode = \"JAX\"\n",
        "\n",
        "# Try to set up MLX mode\n",
        "try:\n",
        "    from pytensor.link.mlx import MLXLinker\n",
        "    import mlx.core as mx\n",
        "    mlx_optimizer = RewriteDatabaseQuery(include=[\"mlx\"], exclude=[])\n",
        "    pytensor_mlx_mode = \"MLX\"\n",
        "    MLX_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MLX_AVAILABLE = False\n",
        "\n",
        "def timer_jax(func, N=1000):\n",
        "    \"\"\"Time function execution with proper JAX synchronization, repeated N times\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        times = []\n",
        "        for _ in range(N):\n",
        "            start = time.perf_counter()\n",
        "            result = func(*args, **kwargs)\n",
        "            if hasattr(result, 'block_until_ready'):\n",
        "                result.block_until_ready()\n",
        "            elif isinstance(result, (list, tuple)):\n",
        "                for r in result:\n",
        "                    if hasattr(r, 'block_until_ready'):\n",
        "                        r.block_until_ready()\n",
        "            end = time.perf_counter()\n",
        "            times.append(end - start)\n",
        "        \n",
        "        mean_time = np.mean(times)\n",
        "        std_time = np.std(times)\n",
        "        return result, mean_time, std_time\n",
        "    return wrapper\n",
        "\n",
        "def timer_mlx(func, N=1000):\n",
        "    \"\"\"Time function execution with proper MLX synchronization, repeated N times\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        times = []\n",
        "        for _ in range(N):\n",
        "            start = time.perf_counter()\n",
        "            result = func(*args, **kwargs)\n",
        "            # For MLX, we need to use mx.eval() to force computation\n",
        "            if MLX_AVAILABLE:\n",
        "                if isinstance(result, (list, tuple)):\n",
        "                    mx.eval(*result)\n",
        "                else:\n",
        "                    mx.eval(result)\n",
        "            end = time.perf_counter()\n",
        "            times.append(end - start)\n",
        "        \n",
        "        mean_time = np.mean(times)\n",
        "        std_time = np.std(times)\n",
        "        return result, mean_time, std_time\n",
        "    return wrapper\n",
        "\n",
        "def run_benchmark(N=1000):\n",
        "    \"\"\"Run comprehensive benchmark comparing PyTensor JAX vs MLX backends\"\"\"\n",
        "    import pandas as pd\n",
        "    \n",
        "    sizes = [128, 256, 512, 1024]\n",
        "    results = []\n",
        "    \n",
        "    print(f\"Running benchmarks with N={N} repetitions per test...\")\n",
        "    \n",
        "    for size in sizes:\n",
        "        print(f\"Testing {size}x{size} matrices...\")\n",
        "        \n",
        "        # Generate test matrices with fixed seed for reproducibility\n",
        "        np.random.seed(42)\n",
        "        A = np.random.randn(size, size).astype(np.float32)\n",
        "        B = np.random.randn(size, size).astype(np.float32)\n",
        "        C = np.random.randn(size, size).astype(np.float32)\n",
        "\n",
        "        pt_A = pt.matrix('A', dtype='float32')\n",
        "        pt_B = pt.matrix('B', dtype='float32')  \n",
        "        pt_C = pt.matrix('C', dtype='float32')\n",
        "        result = pt.dot(pt.dot(pt_A, pt_B), pt_C)\n",
        "\n",
        "\n",
        "        f_jax = function([pt_A, pt_B, pt_C], result, mode=pytensor_jax_mode, trust_input=True)\n",
        "        f_mlx = function([pt_A, pt_B, pt_C], result, mode=pytensor_mlx_mode, trust_input=True)\n",
        "        \n",
        "        # === TEST 1: Matrix Multiplication Chain ===\n",
        "        # PyTensor + JAX backend\n",
        "        @timer_jax\n",
        "        def pytensor_jax_matmul():\n",
        "            return f_jax(A, B, C)\n",
        "        \n",
        "        # PyTensor + MLX backend\n",
        "        @timer_mlx\n",
        "        def pytensor_mlx_matmul():\n",
        "            if not MLX_AVAILABLE:\n",
        "                return None, float('inf'), 0\n",
        "            return f_mlx(A, B, C)\n",
        "        \n",
        "        # Run matrix multiplication test\n",
        "        _, jax_mean, jax_std = pytensor_jax_matmul()\n",
        "        try:\n",
        "            _, mlx_mean, mlx_std = pytensor_mlx_matmul()\n",
        "        except Exception as e:\n",
        "            print(f\"MLX matmul error: {e}\")\n",
        "            mlx_mean, mlx_std = float('inf'), 0\n",
        "        \n",
        "        results.append({\n",
        "            'Size': f'{size}x{size}',\n",
        "            'Operation': 'Matrix Chain (A @ B @ C)',\n",
        "            'PyTensor+JAX Mean (s)': f'{jax_mean:.6f}',\n",
        "            'PyTensor+JAX Std (s)': f'{jax_std:.6f}',\n",
        "            'PyTensor+MLX Mean (s)': f'{mlx_mean:.6f}' if mlx_mean != float('inf') else 'Error',\n",
        "            'PyTensor+MLX Std (s)': f'{mlx_std:.6f}' if mlx_mean != float('inf') else 'N/A',\n",
        "            'MLX Speedup': f'{jax_mean/mlx_mean:.2f}x' if mlx_mean != float('inf') and mlx_mean > 0 else 'N/A'\n",
        "        })\n",
        "        \n",
        "        # === TEST 2: Element-wise Operations ===\n",
        "        # PyTensor + JAX\n",
        "        result = pt.sin(pt_A) + pt.cos(pt_B)\n",
        "        f_jax = function([pt_A, pt_B], result, mode=pytensor_jax_mode, trust_input=True)\n",
        "        f_mlx = function([pt_A, pt_B], result, mode=pytensor_mlx_mode, trust_input=True)\n",
        "\n",
        "        @timer_jax\n",
        "        def pytensor_jax_elemwise():\n",
        "            return f_jax(A, B)\n",
        "        \n",
        "        # PyTensor + MLX\n",
        "        @timer_mlx\n",
        "        def pytensor_mlx_elemwise():\n",
        "            if not MLX_AVAILABLE:\n",
        "                return None, float('inf'), 0\n",
        "            return f_mlx(A, B)\n",
        "        \n",
        "        # Run element-wise test\n",
        "        _, jax_mean, jax_std = pytensor_jax_elemwise()\n",
        "        try:\n",
        "            _, mlx_mean, mlx_std = pytensor_mlx_elemwise()\n",
        "        except Exception as e:\n",
        "            print(f\"MLX elemwise error: {e}\")\n",
        "            mlx_mean, mlx_std = float('inf'), 0\n",
        "        \n",
        "        results.append({\n",
        "            'Size': f'{size}x{size}',\n",
        "            'Operation': 'Element-wise (sin(A) + cos(B))',\n",
        "            'PyTensor+JAX Mean (s)': f'{jax_mean:.6f}',\n",
        "            'PyTensor+JAX Std (s)': f'{jax_std:.6f}',\n",
        "            'PyTensor+MLX Mean (s)': f'{mlx_mean:.6f}' if mlx_mean != float('inf') else 'Error',\n",
        "            'PyTensor+MLX Std (s)': f'{mlx_std:.6f}' if mlx_mean != float('inf') else 'N/A',\n",
        "            'MLX Speedup': f'{jax_mean/mlx_mean:.2f}x' if mlx_mean != float('inf') and mlx_mean > 0 else 'N/A'\n",
        "        })\n",
        "        \n",
        "        # === TEST 3: Matrix Addition with Broadcasting ===\n",
        "        # PyTensor + JAX\n",
        "        result = pt_A + pt_B.T\n",
        "        f_jax = function([pt_A, pt_B], result, mode=pytensor_jax_mode, trust_input=True)\n",
        "        f_mlx = function([pt_A, pt_B], result, mode=pytensor_mlx_mode, trust_input=True)\n",
        "        @timer_jax\n",
        "        def pytensor_jax_broadcast():\n",
        "            return f_jax(A, B)\n",
        "        \n",
        "        # PyTensor + MLX\n",
        "        @timer_mlx\n",
        "        def pytensor_mlx_broadcast():\n",
        "            if not MLX_AVAILABLE:\n",
        "                return None, float('inf'), 0\n",
        "            return f_mlx(A, B)\n",
        "        \n",
        "        # Run broadcasting test\n",
        "        _, jax_mean, jax_std = pytensor_jax_broadcast()\n",
        "        try:\n",
        "            _, mlx_mean, mlx_std = pytensor_mlx_broadcast()\n",
        "        except Exception as e:\n",
        "            print(f\"MLX broadcast error: {e}\")\n",
        "            mlx_mean, mlx_std = float('inf'), 0\n",
        "        \n",
        "        results.append({\n",
        "            'Size': f'{size}x{size}',\n",
        "            'Operation': 'Broadcasting (A + B.T)',\n",
        "            'PyTensor+JAX Mean (s)': f'{jax_mean:.6f}',\n",
        "            'PyTensor+JAX Std (s)': f'{jax_std:.6f}',\n",
        "            'PyTensor+MLX Mean (s)': f'{mlx_mean:.6f}' if mlx_mean != float('inf') else 'Error',\n",
        "            'PyTensor+MLX Std (s)': f'{mlx_std:.6f}' if mlx_mean != float('inf') else 'N/A',\n",
        "            'MLX Speedup': f'{jax_mean/mlx_mean:.2f}x' if mlx_mean != float('inf') and mlx_mean > 0 else 'N/A'\n",
        "        })\n",
        "    \n",
        "    # Create and display results table\n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n",
        "\n",
        "def main(N=1000):\n",
        "    \"\"\"Main benchmark execution\"\"\"\n",
        "    # Display system info\n",
        "    system_info = {\n",
        "        'JAX version': jax.__version__,\n",
        "        'PyTensor version': pytensor.__version__,\n",
        "        'MLX Available': 'Yes' if MLX_AVAILABLE else 'No',\n",
        "        'Platform': 'Apple Silicon' if MLX_AVAILABLE else 'Generic',\n",
        "        'Repetitions (N)': N\n",
        "    }\n",
        "    \n",
        "    if MLX_AVAILABLE:\n",
        "        system_info['MLX version'] = mx.__version__\n",
        "    \n",
        "    import pandas as pd\n",
        "    info_df = pd.DataFrame([system_info])\n",
        "    \n",
        "    # Then run benchmarks\n",
        "    results_df = run_benchmark(N=N)\n",
        "    \n",
        "    return info_df, results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running benchmarks with N=100 repetitions per test...\n",
            "Testing 128x128 matrices...\n",
            "Testing 256x256 matrices...\n",
            "Testing 512x512 matrices...\n",
            "Testing 1024x1024 matrices...\n"
          ]
        }
      ],
      "source": [
        "iteration=100\n",
        "_, results = main(N=iteration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Benchmark Results over 100 repetitions:\n",
            "     Size                      Operation PyTensor+JAX Mean (s) PyTensor+JAX Std (s) PyTensor+MLX Mean (s) PyTensor+MLX Std (s) MLX Speedup\n",
            "  128x128       Matrix Chain (A @ B @ C)              0.000131             0.000300              0.000283             0.000216       0.46x\n",
            "  128x128 Element-wise (sin(A) + cos(B))              0.000104             0.000304              0.000209             0.000145       0.50x\n",
            "  128x128         Broadcasting (A + B.T)              0.000037             0.000296              0.000215             0.000153       0.17x\n",
            "  256x256       Matrix Chain (A @ B @ C)              0.000394             0.000372              0.000441             0.000239       0.89x\n",
            "  256x256 Element-wise (sin(A) + cos(B))              0.000247             0.000389              0.000255             0.000168       0.97x\n",
            "  256x256         Broadcasting (A + B.T)              0.000063             0.000329              0.000217             0.000153       0.29x\n",
            "  512x512       Matrix Chain (A @ B @ C)              0.001004             0.000255              0.000399             0.000188       2.51x\n",
            "  512x512 Element-wise (sin(A) + cos(B))              0.000664             0.000328              0.000263             0.000163       2.53x\n",
            "  512x512         Broadcasting (A + B.T)              0.000115             0.000339              0.000254             0.000156       0.45x\n",
            "1024x1024       Matrix Chain (A @ B @ C)              0.005281             0.000359              0.000993             0.000342       5.32x\n",
            "1024x1024 Element-wise (sin(A) + cos(B))              0.002595             0.000359              0.000408             0.000220       6.36x\n",
            "1024x1024         Broadcasting (A + B.T)              0.000501             0.000346              0.000385             0.000155       1.30x\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nBenchmark Results over {iteration} repetitions:\")\n",
        "print(results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Detailed MLX Timing Analysis ===\n",
            "Compilation time: 0.0020s\n",
            "First execution: 0.0061s\n",
            "Average execution (5 runs): 0.0004s ± 0.0001s\n",
            "Individual execution times: ['0.0007', '0.0005', '0.0006', '0.0008', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0005', '0.0006', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0005', '0.0004', '0.0004', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0007', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0012', '0.0010', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0005', '0.0005', '0.0005', '0.0006', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0008', '0.0006', '0.0006', '0.0005', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0005', '0.0005', '0.0004', '0.0005', '0.0005', '0.0004', '0.0004', '0.0006', '0.0006', '0.0006', '0.0006', '0.0007', '0.0006', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0006', '0.0005', '0.0004', '0.0005', '0.0005', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0005', '0.0005', '0.0006', '0.0005', '0.0005', '0.0004', '0.0005', '0.0005', '0.0008', '0.0006', '0.0006', '0.0007', '0.0006', '0.0006', '0.0006', '0.0006', '0.0007', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0009', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0008', '0.0007', '0.0005', '0.0005', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0006', '0.0007', '0.0007', '0.0006', '0.0008', '0.0007', '0.0006', '0.0006', '0.0007', '0.0007', '0.0006', '0.0006', '0.0007', '0.0007', '0.0007', '0.0007', '0.0006', '0.0007', '0.0007', '0.0007', '0.0007', '0.0008', '0.0007', '0.0007', '0.0007', '0.0006', '0.0007', '0.0008', '0.0007', '0.0008', '0.0007', '0.0007', '0.0008', '0.0007', '0.0007', '0.0007', '0.0007', '0.0008', '0.0006', '0.0007', '0.0007', '0.0008', '0.0007', '0.0007', '0.0007', '0.0007', '0.0007', '0.0007', '0.0006', '0.0007', '0.0007', '0.0007', '0.0008', '0.0007', '0.0007', '0.0007', '0.0007', '0.0007', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0009', '0.0007', '0.0006', '0.0006', '0.0006', '0.0009', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0006', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0004', '0.0005', '0.0007', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0005', '0.0005', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0006', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0007', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0003', '0.0004', '0.0004', '0.0004', '0.0005', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0005', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0005', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0005', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0005', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0004', '0.0004', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0005', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0004', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003', '0.0003']\n"
          ]
        }
      ],
      "source": [
        "# # Additional timing analysis - separate compilation vs execution time\n",
        "# if MLX_AVAILABLE:\n",
        "#     print(\"\\n=== Detailed MLX Timing Analysis ===\")\n",
        "    \n",
        "#     # Test with medium-sized matrix\n",
        "#     np.random.seed(42)\n",
        "#     A = np.random.randn(512, 512).astype(np.float32)\n",
        "#     B = np.random.randn(512, 512).astype(np.float32)\n",
        "#     C = np.random.randn(512, 512).astype(np.float32)\n",
        "    \n",
        "#     # Create PyTensor function (compilation time)\n",
        "#     start = time.perf_counter()\n",
        "#     pt_A = pt.matrix('A', dtype='float32')\n",
        "#     pt_B = pt.matrix('B', dtype='float32')\n",
        "#     pt_C = pt.matrix('C', dtype='float32')\n",
        "#     result_expr = pt_A @ pt_B @ pt_C\n",
        "#     f_mlx = function([pt_A, pt_B, pt_C], result_expr, mode=pytensor_mlx_mode)\n",
        "#     compilation_time = time.perf_counter() - start\n",
        "    \n",
        "#     # First execution (may include additional compilation/optimization)\n",
        "#     start = time.perf_counter()\n",
        "#     result = f_mlx(A, B, C)\n",
        "#     mx.eval(result)  # Force evaluation\n",
        "#     first_exec_time = time.perf_counter() - start\n",
        "    \n",
        "#     # Subsequent executions (should be faster)\n",
        "#     exec_times = []\n",
        "#     for _ in range(1000):\n",
        "#         start = time.perf_counter()\n",
        "#         result = f_mlx(A, B, C)\n",
        "#         mx.eval(result)\n",
        "#         exec_times.append(time.perf_counter() - start)\n",
        "    \n",
        "#     avg_exec_time = np.mean(exec_times)\n",
        "#     std_exec_time = np.std(exec_times)\n",
        "    \n",
        "#     print(f\"Compilation time: {compilation_time:.4f}s\")\n",
        "#     print(f\"First execution: {first_exec_time:.4f}s\")\n",
        "#     print(f\"Average execution (5 runs): {avg_exec_time:.4f}s ± {std_exec_time:.4f}s\")\n",
        "#     print(f\"Individual execution times: {[f'{t:.4f}' for t in exec_times]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlx_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
